diff --git a/omnicons/collators/MaskCollators.py b/omnicons/collators/MaskCollators.py
index bc42de7..5f74338 100644
--- a/omnicons/collators/MaskCollators.py
+++ b/omnicons/collators/MaskCollators.py
@@ -52,3 +52,61 @@ class NodeWordMaskCollator(BaseCollator):
         x[mask] = torch.tensor([self.mask_id])
         y[~mask] = -100  # non-masked words are ignored
         return {"x": x, self.mask_name: y}
+
+
+@dataclass
+class EdgeWordMaskCollator(BaseCollator):
+    # in word graphs, each node is represented by a word
+    # random subset of words are replaced with masked nodes
+    # mask_name should correspond to a SingleEdgeLabelClassificationHead
+    mask_id: int = 1
+    p: int = 0.15
+    mask_name: str = "edge_mask"
+    apply_batch: bool = False
+    edge_types_to_consider: tuple = ()
+
+    def prepare_individual_data(
+        self, data: Union[Data, HeteroData]
+    ) -> Union[Data, HeteroData]:
+        data = data.clone()
+        if isinstance(data, Data):
+            out = self.process(data.edge_index, data.edge_attr)
+            for k, v in out.items():
+                setattr(data, k, v)
+        elif isinstance(data, HeteroData):
+            for edge_type in data.edge_types:
+                if edge_type not in self.edge_types_to_consider:
+                    continue
+                out = self.process(
+                    data[edge_type].edge_index, data[edge_type].edge_attr
+                )
+                for k, v in out.items():
+                    setattr(data[edge_type], k, v)
+
+        return data
+
+    def process(
+        self, edge_index: torch.Tensor, edge_attr: torch.Tensor
+    ) -> Dict[str, torch.Tensor]:
+        links = edge_index.transpose(-1, -2)
+        l = links.shape[0]
+        # generate mask
+        masked_nodes = int(l * self.p)
+        mask = torch.cat(
+            [
+                torch.ones(masked_nodes, dtype=torch.bool),
+                torch.zeros(l - masked_nodes, dtype=torch.bool),
+            ]
+        )
+        mask = mask.index_select(0, torch.randperm(mask.shape[0]))
+        indices = mask.nonzero().reshape(-1)
+        # introduce mask nodes
+        y = edge_attr.reshape(-1).clone()
+        y = y.index_select(0, indices)
+        links = links.index_select(0, indices)
+        edge_attr[mask] = torch.tensor([self.mask_id])
+        return {
+            "edge_attr": edge_attr,
+            self.mask_name: y,
+            f"{self.mask_name}_links": links,
+        }
diff --git a/omnicons/configs/HeadConfigs.py b/omnicons/configs/HeadConfigs.py
index fe2dd00..fa91b68 100644
--- a/omnicons/configs/HeadConfigs.py
+++ b/omnicons/configs/HeadConfigs.py
@@ -3,6 +3,9 @@ from typing import List, Optional, Tuple, Union
 from torch import nn
 
 from omnicons.configs.Config import ConfigTemplate
+from omnicons.models.heads.EdgeClassification import (
+    SingleLabelEdgeClassificationHead,
+)
 from omnicons.models.heads.NodeClassification import (
     MultiLabelNodeClassificationHead,
     SingleLabelNodeClassificationHead,
@@ -73,4 +76,33 @@ class SiameseGraphClsTaskHeadConfig(ConfigTemplate):
         return SiameseGraphClassificationHead(**self.properties)
 
 
-HeadConfig = Union[NodeClsTaskHeadConfig, SiameseGraphClsTaskHeadConfig]
+class EdgeClsTaskHeadConfig(ConfigTemplate):
+
+    def __init__(
+        self,
+        hidden_size: int = 768,
+        hidden_dropout_prob: float = 0.1,
+        num_labels: int = 2,
+        class_weight: Optional[List[float]] = None,
+        analyze_inputs: List[str] = ["a"],
+        edge_type: tuple = None,
+    ):
+        super().__init__(
+            base="EdgeClsTaskHead",
+            properties={
+                "hidden_size": hidden_size,
+                "hidden_dropout_prob": hidden_dropout_prob,
+                "num_labels": num_labels,
+                "class_weight": class_weight,
+                "analyze_inputs": analyze_inputs,
+                "edge_type": edge_type,
+            },
+        )
+
+    def get_model(self) -> nn.Module:
+        return SingleLabelEdgeClassificationHead(**self.properties)
+
+
+HeadConfig = Union[
+    NodeClsTaskHeadConfig, SiameseGraphClsTaskHeadConfig, EdgeClsTaskHeadConfig
+]
diff --git a/omnicons/datasetprep/__init__.py b/omnicons/datasetprep/__init__.py
index 40baf13..a4a2aeb 100644
--- a/omnicons/datasetprep/__init__.py
+++ b/omnicons/datasetprep/__init__.py
@@ -133,8 +133,11 @@ def prepare_reaction_ec_dataset(
     rxn_dataset_fp: str = f"{dataset_dir}/reaction_ec.csv",
     output_dir: str = f"{dataset_dir}/reaction_ec_tensors",
 ):
+    from Bloom.BloomRXN.inference.Pipeline import (
+        get_atom_vocab,
+        get_bond_vocab,
+    )
     from Bloom.BloomRXN.inference.Preprocess import rxnsmiles2tensor
-    from Bloom.BloomRXN.utils import get_atom_vocab, get_bond_vocab
 
     from omnicons.class_dicts import get_ec_class_dict
 
@@ -156,29 +159,29 @@ def prepare_reaction_ec_dataset(
         output_fp = f"{output_dir}/{split}/{reaction_id}.pt"
         if os.path.exists(output_fp):
             continue
-        data = rxnsmiles2tensor(
-            rxn_smiles,
-            atom_vocab=atom_vocab,
-            bond_vocab=bond_vocab,
-        )
-        data.graphs["a"].ec1 = torch.LongTensor(
-            [ec1_class_dict.get(rec["ec1"], -100)]
-        )
-        data.graphs["a"].ec2 = torch.LongTensor(
-            [ec2_class_dict.get(rec["ec2"], -100)]
-        )
-        data.graphs["a"].ec3 = torch.LongTensor(
-            [ec3_class_dict.get(rec["ec3"], -100)]
-        )
-        torch.save(data, output_fp)
+        try:
+            data = rxnsmiles2tensor(
+                rxn_smiles,
+                atom_vocab=atom_vocab,
+                bond_vocab=bond_vocab,
+            )
+            data.ec1 = torch.LongTensor([ec1_class_dict.get(rec["ec1"], -100)])
+            data.ec2 = torch.LongTensor([ec2_class_dict.get(rec["ec2"], -100)])
+            data.ec3 = torch.LongTensor([ec3_class_dict.get(rec["ec3"], -100)])
+            torch.save(data, output_fp)
+        except:
+            print(rec)
 
 
 def prepare_reaction_ec_fewshot_dataset(
     rxn_dataset_fp: str = f"{dataset_dir}/reaction_ec_fewshot_dataset",
     output_dir: str = f"{dataset_dir}/reaction_ec_fewshot_tensors",
 ):
+    from Bloom.BloomRXN.inference.Pipeline import (
+        get_atom_vocab,
+        get_bond_vocab,
+    )
     from Bloom.BloomRXN.inference.Preprocess import rxnsmiles2tensor
-    from Bloom.BloomRXN.utils import get_atom_vocab, get_bond_vocab
 
     from omnicons.class_dicts import get_ec_class_dict
 
diff --git a/omnicons/datasets/.gitignore b/omnicons/datasets/.gitignore
index f7bf61e..034ad3d 100644
--- a/omnicons/datasets/.gitignore
+++ b/omnicons/datasets/.gitignore
@@ -1,4 +1,5 @@
 reaction_fewshot_ec.csv
 molecular_graphs/*
 bloom_dos_annotations/*
-bgc_graphs/*
\ No newline at end of file
+bgc_graphs/*
+reaction_ec_tensors/*
\ No newline at end of file
diff --git a/test.py b/test.py
index ec07dd3..9d30342 100644
--- a/test.py
+++ b/test.py
@@ -1,3 +1,7 @@
-from omnicons.datasetprep import prepare_bgc_graphs
+from omnicons.datasetprep import (
+    prepare_reaction_ec_dataset,
+    prepare_reaction_ec_fewshot_dataset,
+)
 
-prepare_bgc_graphs()
+prepare_reaction_ec_dataset()
+prepare_reaction_ec_fewshot_dataset()
diff --git a/training/BloomRXN/ECFewShotTraining/models.py b/training/BloomRXN/ECFewShotTraining/models.py
index c38aa40..14a4104 100644
--- a/training/BloomRXN/ECFewShotTraining/models.py
+++ b/training/BloomRXN/ECFewShotTraining/models.py
@@ -155,9 +155,7 @@ def get_model(
     node_encoder_config = get_node_encoder(
         atom_vocab, embedding_dim=embedding_dim
     )
-    edge_encoder_config = get_edge_encoder(
-        bond_vocab, embedding_dim=embedding_dim
-    )
+    edge_encoder_config = get_edge_encoder(bond_vocab)
     gnn_config = get_gnn(embedding_dim=embedding_dim)
     transformer_config = get_transformer(embedding_dim=embedding_dim)
     graph_pooler_config = get_graph_pooler(embedding_dim=embedding_dim)
diff --git a/training/BloomRXN/ECTraining/models.py b/training/BloomRXN/ECTraining/models.py
index dd98904..f47b8f6 100644
--- a/training/BloomRXN/ECTraining/models.py
+++ b/training/BloomRXN/ECTraining/models.py
@@ -140,9 +140,7 @@ def get_model(
     node_encoder_config = get_node_encoder(
         atom_vocab, embedding_dim=embedding_dim
     )
-    edge_encoder_config = get_edge_encoder(
-        bond_vocab, embedding_dim=embedding_dim
-    )
+    edge_encoder_config = get_edge_encoder(bond_vocab)
     gnn_config = get_gnn(embedding_dim=embedding_dim)
     transformer_config = get_transformer(embedding_dim=embedding_dim)
     graph_pooler_config = get_graph_pooler(embedding_dim=embedding_dim)
diff --git a/training/BloomRXN/MLMTraining/DataModule.py b/training/BloomRXN/MLMTraining/DataModule.py
index 3bffd05..1022768 100644
--- a/training/BloomRXN/MLMTraining/DataModule.py
+++ b/training/BloomRXN/MLMTraining/DataModule.py
@@ -1,6 +1,6 @@
 from typing import Dict
 
-from Bloom.BloomRXN.utils import get_atom_vocab, get_bond_vocab
+from Bloom.BloomRXN.inference.Pipeline import get_atom_vocab, get_bond_vocab
 from pytorch_lightning import LightningDataModule
 from torch.utils.data import DataLoader
 
diff --git a/training/BloomRXN/MLMTraining/models.py b/training/BloomRXN/MLMTraining/models.py
index 0b9c46f..9e16750 100644
--- a/training/BloomRXN/MLMTraining/models.py
+++ b/training/BloomRXN/MLMTraining/models.py
@@ -1,7 +1,6 @@
-from typing import Callable, Optional
+from typing import Callable
 
-import torch
-from Bloom.BloomRXN.utils import get_atom_vocab, get_bond_vocab
+from Bloom.BloomRXN.inference.Pipeline import get_atom_vocab, get_bond_vocab
 from torch.nn import ModuleDict
 
 from omnicons.lightning.GraphModelForMultiTask import (
@@ -116,9 +115,7 @@ def get_model(
     node_encoder_config = get_node_encoder(
         atom_vocab, embedding_dim=embedding_dim
     )
-    edge_encoder_config = get_edge_encoder(
-        bond_vocab, embedding_dim=embedding_dim
-    )
+    edge_encoder_config = get_edge_encoder(bond_vocab)
     gnn_config = get_gnn(embedding_dim=embedding_dim)
     transformer_config = get_transformer(embedding_dim=embedding_dim)
     graph_pooler_config = get_graph_pooler(embedding_dim=embedding_dim)
diff --git a/training/BloomRXN/MLMTraining/train.py b/training/BloomRXN/MLMTraining/train.py
index ea2ce60..e31a32c 100644
--- a/training/BloomRXN/MLMTraining/train.py
+++ b/training/BloomRXN/MLMTraining/train.py
@@ -21,8 +21,9 @@ def train(
     # setup directories
     os.makedirs(checkpoint_dir, exist_ok=True)
     # data module
-    dm = ReactionDataModule()
-    dm.setup()
+    dm = ReactionDataModule(
+        data_dir="/home/gunam/storage/workspace/bearlinker_workspace/zenodo/reaction_ec_tensors"
+    )
     # model
     model = get_model(embedding_dim=embedding_dim)
     # trainer
@@ -51,7 +52,7 @@ parser.add_argument(
 parser.add_argument(
     "-logger_entity",
     help="wandb entity",
-    default="user",
+    default="magarvey",
 )
 parser.add_argument(
     "-logger_name",
